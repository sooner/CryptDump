\section{Introduction}


With the development of cloud computing, more services are deployed in the cloud. Security is the first issue to taken into consideration when people store their data in the cloud. Encryption is an important technique to ensure data security. However, for a database system, when data is encrypted, it could be hard for people to perform operations on those data. Fortunately, with the help of certain encryption schemes, computing over encrypted data is possible. Cryptedb\citep{popa2011cryptdb}, which was proposed in sosp'11, is such kind of system. Similarity system has been implemented by many companies like google,SAP,Microsoft,and some Startups\citep{cryptdbsite} \citep{kerschbaum2013encrypted}.


The idea of computing over encrypted data came from homomphic encryption. Fully Homomorphic enctyption is too expensive\citep{gentry2009fully}, however, to support common database operation, fully homomorphic is not the only way. The main idea of Cryptdb is to use the characteristics of several encryption schemes to support different operations on columns in a relational table.  Since each encryption scheme can only support one operation, to allow multiple operations, each column in the table is duplicated many times according to how many operations each column need to support, and different encryption schemes are applied to each copy. Such data layout allows computing over encrypted data, but incur huge storage overhead at the same time. According to the author\citep{popa2011cryptdb}, experiments with TPC-C showed an increase of 3.76 times of storage size.

Another import issue of a cloud database system is data intergrety. The system may suffer from disk failures or system crash, so database backup is needed for such systems. In fact, operators of a database system may come across problems like Operating system crash, power failure, file system crash or Hardware problem\citep{mysqlbackupdocumentation}.


This storage overhead is inevitable when we want to support different operations. However, for a data backup, this duplication is unnecessary since we do not perform operations on the backup.

To reduce storage overhead in backup, data deduplication techniques are ofthen used. In fact, data deduplication has become a standard component for backup systems\citep{fu2015design}. However, for cryptdb, traditional deduplication methods are limited since it could be hard to deduplicate encrypted data, and those methods are unaware of the relations between encrypted data. There are research about deduplicating encrypted data, but they focus on user files in the cloud instead of database systems. 

Cryptedb records how the original tables are encrypted and how the columns are replicated. We call those data metadata in this paper. It stores metadata on the proxy. If we introduce this kind of extra information, data deduplication for backup system can be more efficient. And also, the onion layout of fields allow us to make tradeoff between storage size, recover time, and data security.


In this paper, we raise the question that how can we reduce the the overhead of storage with the help of metadata, and that how to use the information to make proper tradeoff. We should consider storage size, security and the time consumed for data recovery. In our work, we propose a new data deduplication method for Cryptdb, which utilizes metadata information to help find duplicates in encrypted data. We implemented a tool called Cryptdbdump and did experiments with TPC-C-MySQL in the newest version of Cryptdb.


To realise this, we need to address three main challenges:

1) How to parse the metadata of Cryptdb

2) How  does the time and space overhead change as the onions and layers change

3) How to choose the right onion and layer to make optimal backup


To tackle those problems, we need to

1) analyse the structure of metadata in Cryptdb and find duplicates

2) do analysis on different onions and layers of encryption scheme and provide statiscis

3) design a strategy for data backup with the help of metadata and the costs information 

To summarize, we make the following contributions.

1) A full analysis of the cost of each layer and onion in Cryptdb

2) A new data deduplication method for Cryptdb that utilizes metadata information 

3) A strategy for choosing the right portion of data for backup, which taken storage size, 

4) An implementation of data backup tool called Cryptedbdump 

To our knowledge, we are the first to propose a deduplication method for Cryptdb with the help of metadata to reduce storage overhead, and our high level deduplication method can be used together with traditional deduplication techniques like block level comparision \citep{bellare2013message}.

The rest of this paper is structured as follows. In section 2, we introduce the structure of cryptdb and the structure of encryption metadata in the newest version of Cryptdb. Then, we introduction the characteristics of the encryption schemes use in Cryptdb in section 3. And in section 4, we introduce popular data backup and deduplication methods. In section 5, we introduce our design details and results of experiments. In section 6 we introduce related work. And Section 7 concludes the paper.
\\ \\ \\ 





\section{Introduction to Cryptdb}

This section introduces the structure of Cryptdb to show how data is duplicated. This introduction is based on the original paper and the newest verion of Cryptdb implementation. We would like to introduce the following content.

1. structure of client, proxy, and MySQL server\\
2. structure of metadata storage\\
3. datalayout \\

This section will give you insight on how we find duplicates in Cryptdb, and important issues to consider when designing a backup system.

\subsection{The client,server, proxy structure}

In the cryptdb system design, there are three identities, client, server, and proxy. The Client is the webserver, and the server has MySQL installed. The client communicate with the proxy instead of with the server. The MySQL server is not trusted while the Proxy is trusted. When the client receive DDL queries, it will stored related metadata to describe the encryption schema of each table. For both DML and DDL queries, the proxy perform query rewrite to encrypt the query and send encrypted query to MySQL server. The results are sent to the proxy first for decryption, and then plain text will be sent to client. Figure~\ref{fig:stack1} shows the basic structure of Cryptdb and the traditional database. 


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/Cryptdb.png}
\caption{structure of cryptdb}
\label{fig:stack1}
\end{figure}



\subsection{metadata layout and structure}

This section talks about metadata layout and the conresponding encryption scheme. Those scheme will impact how we make choices when we bakcup data. The metadata form a hirechy from the database level to the encryption layer level. It has a structure of a tree. Figure~\ref{fig:stack2} shows the structure of onionmeta.


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/layers.png}
\caption{Onionmeta layout}
\label{fig:stack2}
\end{figure}

Every node in the tree is storaed as a record with a unique id in a relational table. The relation among nodes and the content of a node is also stored in the same table. Each onion represents an encryption scheme, which has many layers. This tree structure can be stored in a simple relational table, 

CREATE TABLE MetaObject(id bigint(20) unsigned auto\_increment, parent\_id bigint(20),serial\_key varbinary(500),serial\_object varbinary(500));  


So the metadata of databases, tables, fields, onions, layers are all serialized and stored in the proxy. Since the amount of data is related only to the number tables and the number of fields in those tables, the storage size is small. Also, metadata is needed for decryption and data recovery, so we use physical backup and do not deduplicate the metadata. Metadata is in the form is plaintext, so traditional deduplication techinques can work properly.



\section{Encryption schemes}


\subsection{data layers for string type}

For string type, the newest version of cryptdb only support two onions: DET and OPE. Search was removed. Also, since the onion search has only one layer and it's quit easy to do analysis, we focus on the other two layers
in this section. For onion DET, the DET layers uses 128bits AEC-CMC encryption, which is a simple wrapper function of AES-CBC. AES-CBC will pad the plaintext if the size of plaintext can not be divided by the block size. For simplicity, in the current inplementation, if the input can be diveded by the block size, the size of the datatype is expanded by one block size. So, as we add more layers to the onion, the size of the datatype keeps increasing. 

For OPE-str, the data type will be transformed to 64 bits integer. Therefore, the characteristic of OPE-str is constrained to 64bits. The for the RND layer, blowfish will be used since the data type is transformed to integer. OPE is used only for comparing and sinced it loses information, we can not decrypt ope-str. So this onion is not suitable for backup. 

Figure 3.2 provides illustration of the onions. 



\subsection{data layers for integer}

For integer type, cryptdb create three onion: DET, OPE, and HOM. Figure 3.1 shows the layout of the onions. Hom expand any type of integer to 2048 bits binarychar. For DET and OPE, the type conversion is more complex. DET layer uses blowfish encryption, which requires 64bits integer. RND for integer also uses blowfish encryption. So for DET onion, any integer will be at most expanded to 64 bits long. For onion OPE, the first layer requires that the size of cipher text is twice the size of plaintext. If the size of ciphertext is larger than 64bits thus unable to fit into an integer type, varbinary is used instead. 



\subsection{data layout and structure}

In this section, we talk about the onion layout and How an original table is expanded whith a specific onionlayout. Since the only two data type supported for encryption, we will use and imginery table with those two data types. 



\begin{figure}[tb]
\centering
\includegraphics[width=8cm]{images/Onions.jpg}
\caption{Onion encryption layers for integer type}
\label{fig:stack3}
\end{figure}


\section{database backup methods and deduplication}

Data backup is an important functionality of database systems. All the popular database systems have backup methods. Based on how the backup is stored, we can can have two categorities of backup methods: logical backup and physical backup. Backup tools focus on low storage overhead and fast recovery. Since we do experiment with MySQL, this section focus on backup methods for MySQL. Other systems will be similar. We use backup to deal with disasters. In this section, we introduce popular backup methods. Typical backup methods also provide functions like encryption for security and compression for saving disk space.GPG tools is used for encryption.



\subsection{logical backup}

Logical backup save information in the form of SQL queries \citep{mysqlbackupdocumentatio}. For Logical backup, it's easy to control the backup granularity, and it's highly portable since the backup is in text format. MySQLdump and MySQLdumper\citep{mysqldumpper} are examples of Logical backup tools. We can also use SELECT .. INTO OUTFILE to create delimited-text files for logical backup. The basic process of logical backup is first to use select queries to pull data from the MySQL-server, and then use the data to construct Insert queries and save them into a text file. Also, Create table queries will be saved into files for recovery. We can also categorize backup method by other creterions, since we can about storage size and the logical deduplcation, we only discuss two types of backup here.





Here we describe a typical logical backup command.


\begin{itemize}
\item[--] mkdir /backups/mysqldump
\item[--] mysqldump --single-transaction --all-databases |gzip > /backups/mysqldump/today.sql.gz 
\end{itemize}






                                                 

As we can see, we can create a directory and use simple mysqldump command and options to backup our data into a file. Also, compression methods like gzip are ofthen used.

Also, If you use mydumper, you can use the following commands.

\begin{itemize}
\item[--] mkdir -p /backups/mydumper/today
\item[--] mydumper --outputdir=/backups/mydumper/today --host=localhost --compress --kill-long-queries --verbose=3 --build-empty-files --triggers --events -routines 
\end{itemize}

 
To recover the data, we can just uncompress the data and executre the sql query in the .sql file. 

you can also use binlog for backup and recovery.

\begin{itemize}
\item[--] mkdir -p /backups/binlogs
\item[--] cd /backups/binlogs
\item[--] mysqlbinlog --raw --read-from-remote-server --stop-never --verify-binlog-checksum --host=192.168.56.201 --stop-never-slave-server-id=999 mysql-bin.000001
\end{itemize}


\subsection{physical backup}

Physical bakcup consists of raw copies of directories and files that store the database contents\citep{mysqlbackupdocumentation}. In fact, simple commands like cp can be used as physical backup method. Popular physical backup tools includes ibbackup and XtraBackup\citep{xtrabackup}. MySQL store data in a set of files in a directory. Typical files includes .
This type of backup is fast since they do not convert data into logical form. Since the metadata show the data in logical form, we choose to design our strategy based on logical backup. 


Here we describe a typical physical backup workflow. 

(to be added)

The recovery form physical backup is easy. Just to uncompress the backup files and then move it the the mysql directory. 


\begin{itemize}
\item[--] service mysql stop
\item[--] rm -rf /data/mysql
\item[--] cd /backups/mylvmbackup
\item[--] tar zxf <backup file>
\item[--] mv backup/mysql /data/mysql
\item[--] service mysql start
\end{itemize}


We can find that logical backup allow user to find duplicate in the database directly, so in our design, we give our design based on logical backup.





\section{Our Analysis}

In this Section, we discuss the details of each onion design, and based on those analysis, we propose backup strategies for each data type. The simplest method for backup is to the find the smallest onion and only backup one onion, and later use this onion for recovery. In this discussion, we will focus on backup for each field, and the situation for a table should be the same. The onion 'search' for string is not yes implemented in the newest version of cryptdb, so for this onion, our analysis is based on the descryption in \citep{song2000practical}.

In the design, each field in a table can have several onions, and each onion can have several layers. So for one field, we can make a choice of whether to remove or retain a onion. If one field has three onion, then we can have eight choices. Since we can not remove all of the onions, then there are seven possible choices. For each onion, we can either choose to backup it directly, or to decrypt some layers and then backup it. We will discuss whether thoses choices are practical.

\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/onions-for-string.png}
\caption{onions-for-string}
\label{fig:stack4}
\end{figure}


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/det-str.png}
\caption{det-str}
\label{fig:stack5}
\end{figure}



Figure~\ref{fig:stack4},~\ref{fig:stack5} shows how the size of a string type change for each onion and for each layers of the onions. As the firgure depicts, for the onion DET, the plaintext is 10Bytes long, while the size of the onion in layer RND is 48 Bytes long. That because the onion DET has three layers, namely DET-JOIN, DET, and DET-RND. The first two layers uses AES-CMC for encryption, and the third layer uses AES\_CBC. In the current implementation, block size of 16 bytes is uses. In current version of Cryptdb,  ff the input text of the algorithm can not be devided exactly by the block size, then the input should be padded with zero to make it divisible. If the input text is divisible, cryptdb will pad the text by one block. So, for plain text with size 10 Bytes, after it is encrypted with layer DET-JOIN, it becomes 16 Bytes. After it is encryptdb with layer DET, it is padded to 32 Bytes. And when it is in layers DET-RND, the size becomes 48 Bytes. Note that for ciphertext, cryptdb used only the integer type of MySQL if the size of ciphertext can fit in that data type. Otherwise, varbinary is used instead. Since Onion Search is not implemented properly, we do not include it in the figure. In fact, the onion search is quiet simple: it has only two layers, and according to \citep{song2000practical}, the size of ciphertext is roughly the same as plaintext. In the current implementation, the change of data type and size of OPE is quiet complex. In fact, the Onion OPE only guarantee the properity of order preserving for 8 bytes prefix of a string.




\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/onions-for-integer.png}
\caption{onions-for-integer}
\label{fig:stack6}
\end{figure}


Figure~\ref{fig:stack6} shows how the storage size change for each onion for integer type. Currently, there are three onions for integer, HOM uses Pailliar algorithm, DET for integer uses 64bits blowfish, so the size of the ciphertext is always 64bits integer. There is no padding. HOM uses pailliar, so the result is aloways 256bytes integer. MySQL do not has such long integer, so varbinary(256) is used instead. Now lets's discussion how the size change across layers of the onion OPE for both String type and Integer type. 



OPE requires that the cipher text size is double the size of plain text. Let's first discuss OPE-INT. This onion has three layers, namely OPE-INT-JION,OPE-INT,OPE-RND. If the plaintext is of the type tinyint, which has only 1 Bytes. Then after the layer OPE-INT-JOIN, it becomes 2 Bytes, and after the layer OPE-INT, it becomes 4 Bytes. So for the layer OPE-RND, the input is a 4 Bytes long integer, and the result of OPE-RND can be represented by a 8 Bytes bigint in MySQL. If the size of the integer is 64 bits, then after the encryption of one layer of OPE, the ciphertext should be 128 bits, which can not fit in an integer type. Cryptdb uses 128 bits varbinary data type. In that case, the input of the layer rnd is string instead of integer, so algorithm AES is used instead of blowfish. So the onion OPE-INT may contain layer RND-STR. 

Then let's talk about OPE-STR. OPE-STR can also have three layers, OPE-STR-JOIN, OPE-STR, OPE-RND. When the input of the algorithm is a string, plaintext length of 4 Bytes and  ciphertext length of 8 Bytes are specified. That is, only the prefix are encrypted and has the properity of order preserving. Then in the layer OPE-STR, since in the first layer, the data type has been transformed to 64bits integer, the OPE-STR is transformed to OPE-INT, and the input is 64bits long. So after this layer, integer can no longer fit ciphertext. The data type has become 128 bits varbinary type. And then in the third layer, aes is used for the layer, adding one block to the size, making the total size of the onion 32 Bytes.


The above discussion reveals two facts:

\begin{itemize}
\item[--] The size of Onion OPE can not exceed 32 Bytes regardless of the size of plaintext
\item[--] Onion OPE can loose information of the plaintext, so we can not use the Onion OPE
\end{itemize}



As we have known the properity of the encryption schemes and the data layout. We can now start to design the strategy of deduplication. We will first discuss each onion and then give three simple strageis for analysis. Then in the experiment section, we will use our workload to test the strategies. We first discuss the design choices for each onion of interger data and string data. 


\begin{itemize}
\item[--] Integer/String OPE: For ope-int, we have little choices, since OPE is not able to be decrypted. If this onion is removed, then we need two OPE-Encrypt operation and one AES-enctypt operation to get it back. 
\item[--] Integer HOM: HOM takes a long time to recover, and also occupy 256 Bytes. If we choose to remove it, then we should use one Paillar-encrypt operation to get it back. Only retaining HOM is never a goog choice, because it takes a long time to decrypt HOM for recovery, and also the storage size is large. So HOM should be removed or be retained together with other onions.
\item[--] Integer /String DET: If this onion is removed, then it takes three blowfish-encrypt operations to recover it. For String type, we need two AES-CMC-encrypt and one AES-CBC-encrypt operation to recover the onion. The encryption time is related to the size of the string.
\item[--] Integer IV: 8Bytes integer. faster to regenerate
\end{itemize}



Since the detailed performance is related to so many things, we do not provide exact strategy for backup. Instead, we summarize several Principals for cryptdb backup and then give some examples. 

\begin{itemize}
\item[--] One original field is replicated several times and encrypted with different schemes
\item[--] If we want mininum storage overhead, then for each field we can find the onion with the least size, like AES or blowfish, that can produce ciphertext of the size similar to plaintext, and remove all other onions. If we want minimum time overhead, we should backup all the onions, so that we can recover the data directly. Other choices are something in between.
\item[--] Metadata should have a full backup for data recovery, and the size of metadata is small. 
\item[--] For recovery, we need to decrypt the onion we have, and based on the metadata, recompute the ciphertext for other onions.
\item[--] For one onion, we can choose the stay in RND layers, which means identical plaintext can have different ciphertext. If we decypt it to a layer that is not random, then identical plaintext can have the same ciphertext. If most of the plain text in this column are the same, then it will be easy to compress this filed to reduce storage overhead. Also, if we decrypt RND layer, then we can remove the IV column, which further reduces storage overhead.
\end{itemize}


Based on the principals, we proposes the following strategies for String data and Integer data. 

\begin{itemize}
\item[--] Remove RND and use DET onion will produce the least storage overhead.
\item[--] Do not remove anything. This is the strategy with the most storage over head and least computation overhead
\item[--] Retain RND and salt for DET, remove HOM and OPE, and Search. This method has high security level.
\end{itemize}




For those two data types and strategy, we construct microbenchmark to do experiments. Actually, the time needed for recovery can be reduced by using multi-threading or other optimization. We assumes that other factors are the same and only condiser the overhead of recomputation. We use single thead whith pipeline insert.

So, the overall strategy looks like the following: we let users choose the strategy bases on the what they care about. They can either choose to backup all the onions, or choose to have minimum backup or something in between. Figure~\ref{fig:stack7} and Figure~\ref{fig:stack8} shows how to encrypt and extend the plain text, and then choose one onion for back up, and then compress the onion. For recovery, we should first decrypt the onions and then recompute the deleted onion. Then we should be able to construct the sql queris for recovery. This is the same as the logical backup recovery method.

\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/Workflow.jpg}
\caption{Backup and compression}
\label{fig:stack7}
\end{figure}


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/Recovery.jpg}
\caption{Aes encryption and decryption time}
\label{fig:stack8}
\end{figure}









\section{experiments}

We evaluate our work in a machine with Intel(R) Xeon (R) CPU E565 2.4GHz in Ubuntu 16.04. 


Since we may want to remove salt, we first need to konw how fast can we generte a new salt. We do experiment with the function in cryptdb code and find that the computation overhead for one salt is about 0.7 us. So this computation overhead is negligible.

We need to know how much time is consumed to do computation in each layer. Also for each strategy, we should give the estimated time overhead for recovery, so we should know how time is consumed for each algorithm, and we give uses the choice of each strategy. So, our strategy is to show customers the reencryption overhead. 


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/aes.png}
\caption{Aes time consumpation}
\label{fig:stack9}
\end{figure}


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/time.png}
\caption{time consumpation}
\label{fig:stack10}
\end{figure}

As we can see in Figure~\ref{fig:stack9} , we can now estimate the time overhead for each strategy and give suggesstions.


We now begin to analysis the two type of data that is supported by cryptedb.

For integer, as we mentioned before, the column are replicated three onions with an additional salt column. We create one simple table with only one field, and backup based on that data, and show the size of each column.


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/sizeofeachonion.png}
\caption{Size of onion for integer type}
\label{fig:stack11}
\end{figure}

Figure~\ref{fig:stack11} shows the strategy and the time overhead for each of the strategy we discussed in the string.


For string type, we do experiment with a simple table of varchar(10). After encryption, the onion DET will be tranformed to varbinary(48) due to the padding, and OPE will be varbinary(32), salt should be big int. We insert 1000000 plaintext of length 10 into the table, and the the following results. 


\begin{figure}[tb]
\centering
\includegraphics[width=4cm]{images/sizeofstr.png}
\caption{size of onion for string type}
\label{fig:stack12}
\end{figure}


We can find that even for very short string, three layer of encryption in det can make it larger than the onion OPE. So ope is always assumed  smaller than DET. and the size of sat remains the same.


We insert TPC-C data into Cryptdb, and use mysqldump to backup data into a file dump1.sql Then we apply the strategy to the backup process, backup data into a second file dump2.sql, and compare the size of dump1.sql and dump2.sql. We also construct microbenchmark that create table with only string type and integer type.



\begin{figure}[tb]
\centering
\includegraphics[width=8cm]{images/tpcc.png}
\caption{tpcc data}
\label{fig:stack13}
\end{figure}


DET is always retained. Based on this, we can estimate the overhead for each strategy. 



In this experiment, Cryptdump only dump DET and IV, so for integer type, we can significantly reduce the amount of storage size. We do not use extended insert, so the command we use to backup data is.


mysqldump --skip-extended-insert -uroot -pletmein -h127.0.0.1 --hex-blob --compact tpcc1000 > back.sql



Recovery time is affected by the implementation details. Here we measure the time for the deduplicated data to be recovered to the original data, and this can be considered as the time overhead for recovery.A fter that, we can use the data for backup.


The following we use tpcc to analysis the three main strategy.


Based on the above analysis on the decryption and encryption time, we can conclude that HOM takes longer time for recovery and also has large storage size. So If we need small storage size, we can choose to remove HOM Onion, whlie having long recovery time. On the other hand, If we do not care about storage size, then we can choose to keep Hom onion. 






\section{implementation}

In this section, we describe briefly the implementation details of our design. 1.The neweset version of cryptdb do not support stmt, and tpcc-use stmt. We do not currently implement stmt for cryptdb, instead, we first insert plaintext data into cryptdb, dump data into a sql file , and then insert data into cryptdb using the data. mysqldump has default extended insert, but too long do not support. So we reconstruct the data to limit the size of extended insert. We only use warehousre two, and modify the datatype that is not supported by cryptdb. Namely, we change date type to string type, float to integer type, signed integer to integer type. TPCC is the standard benchmark, so we still want to show the resutls based on it. The detailed configuration is aviliable at this site.

For the backup choices, we should parse the metadata structure of cryptdb, and specify the command line arguments for it. This function can be implemented in less than 800 lines of C++ Code, including the testing code. Our future work should make cryptdb stable and add new features to it, the backup code can also change due to that kind of change, but the underlining principal for backup still apply.

To remove rnd, we currently do not implement it. We can only mannualy issue queries to cryptdb and then use the adjustment to find the rnd layer. 

search\citep{song2000practical} algorithm O(n) stream cipher and block cipher operation and introduces little space overhead, so we except it to be similar to the onion DET. According to cryptdb paper, the time is longer. So, we can definately remove one duplicate onion and save space.


Current version of cryptdb also remove the layer ope-jion, we implemented it in our version of cryptdb and so the onion ope has three layers. The modified version is available at \citep{practical-cryptdb}. 





\section{related work}

1)database backup dedpulication compression

Deduplication on storage system is a hot research topic\citep{paulo2014survey}.\citep{xu2017online} proposes an online deduplication for operational database system. According to \citep{xu2017online}, dedup using hash method works well for both primary and backup storage data sets that comprises of large files that are rarely modified. In our work, we focus on archivial data, so the files are rarely modified. And those traditional method can work together with our proposed method. There are many work on deduplication for secondary storage, the can either use exact dedup\citep{dubnicki2009hydrastor} or similarity based dedup\citep{xu2015reducing} \citep{aronovich2009design}\citep{you2005deep}.

These traditional chunk based dedup system can not handled encryption well, So there are also work that address the tension between encryption and deduplication. 

Encryption and deduplication are in tension, and there are research about this issue.\citep{bellare2013message} \citep{puzio2015perfectdedup} proposed message-locked enctyption, a kind of convergent encryption to addressed this problem. \citep{yan2016deduplication} addressed the problem of deduplication of encrypted files in a multi-user situation through authorized party and token comparison, they proposed their strategy through ownership chanllenge and proxy-re-encryption. In constrast, our work forcus on generating a backup file that occupy less storage space. In fact, deduplication methods on encrypted data fall into two catagories, convergent encryption and proof of ownership\citep{akhila2016study}. Convergent encryption works like DET, So detjoin level have the potential to allow deduplication.

How to combine those two methods together to make efficient deduplication is future work.

Actually, block level comparision do not work well for database workload, so there are also research targeting database workload. 

\citep{francinasurvey} proposed three encryption schemes for deduplication, those are file level deduplication.
There are also work on solving the problem of compression and encryption\citep{zheng2017minicrypt}.

\citep{mandagere2008demystifying} says that compress tools like gzip reduce intra-file duplication while deduplication reduces both inter and intra file deduplication. It also talked about client side deduplication using metadata.



\section{future work}

\begin{itemize}
\item[--] More efficient homomorphic encryption algorithms other than pailliar
\item[--] Deduplication for master-slave replication
\item[--] Deduplication for distributed database
\end{itemize}



\section{Conclusions}

Fully Homomorphic encryption is still not applicable, So researchers proposed encryption schemes that can support only a small set of operations on encrypted data, and use replication to support many operations at the same time. This idea make homomorphic operation possible in database system. In this paper, we found the problem of storage overhead of such kind of systems, and We study Cryptdb, and propose a backup technique that can utilize metadata to find duplicates in encrypted data. We analysed the details of cryptdb, figured out how to use metadata information to find duplicates in the data. Then we analysed the time and storage overhead of the multi-onion structure, and together with this information, we designed a simple strategy that can reduce storage overhead while incurring little time overhead. We implemented a database backup tool for Cryptedb and experiments showed that our tool can reduce storage overhead without exposing plaintext to administors. We also analysed the tradeoff we can make while making a backup.

For Cryptdb, the metadata stored in a local database in the proxy also need to be backed up. For metadata, the owener of data should get hold of it sinced it stores the encryption keys. The proxy can provide information the the server for save back so that data do not get lost, for recovery, this dump should be give to the proxy for reencryption and recovery.


In our system, we use statics collected, such as time consumpation for encryption and decryption, to help make decisions. Those information can change with systems. So beform doing a backup, we can first run test to fetch such kind of information and then backup the data.

Also, our design allow user to choose from a set of space security and space overhead. We can actually remove rnd for deduplication and compression, which make our system compitable to compression systems.

We can also make conclusions of several encryption algorithms.


\begin{acks}
  The authors would like to everyone for their help, without which this work would not have been possible. 
\end{acks}


